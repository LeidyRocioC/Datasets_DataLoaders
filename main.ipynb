{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos\n",
    "Los Datasets y DataLoaders son funciones de pytorch que nos permites cargas, preprocesar y gestionar sets de datos de manera eficiente en pytorch. Con esto nos aseguramos de procesar nuestro dataset fuera del loop de entrenamiento en pytorch\n",
    "\n",
    "### Datasets\n",
    "Clase la cual crea un objeto que contiene nuestro set de datos, debe contar con las siguientes funciones:\n",
    "- \\_\\_init\\_\\_ : inicializador del objeto\n",
    "- \\_\\_len\\_\\_ : Función que regresa la longitud del set de datos\n",
    "- \\_\\_getitem\\_\\_ : Regresa un elemento del set de datos en forma de tupla, regesa el registro y si etiqueta\n",
    "\n",
    "### DataLoaders\n",
    "Clase la cual crea un objeto que arma los batches de datos y los organiza para entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data tabular\n",
    "Un ejemplo de dataset y dataloaders con datos de un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = os.path.join('.','./data/churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        data = pd.read_csv(data_dir)\n",
    "        self.x, self.y = self.limpieza(data)\n",
    "        self.samples = self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx] # Se regresa un registro de datos junto con su etiqueta\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def limpieza(self,data):\n",
    "        datos_y = data[data.columns[-1]]\n",
    "        datos_x = data.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\", \"Exited\"])\n",
    "        datos_x = pd.get_dummies(datos_x)\n",
    "        escalador = StandardScaler()\n",
    "        datos_x = escalador.fit_transform(datos_x)\n",
    "        x = torch.from_numpy(datos_x).float()\n",
    "        y = torch.from_numpy(datos_y.values).float()\n",
    "        y = y[:,None]\n",
    "        return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un objeto de la clase Dataset_Custom\n",
    "set_datos = Dataset_Custom(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos leer datos de nuestro set de datos con las funciones de la  clase 'Dataset_Custom'\n",
    "set_datos.__getitem__(43) #Se puede buscar por idx\n",
    "# set_datos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un data loader dandole el set de datos, el tamaño de batch, el numero de procesos en paralelo, tomar muestras aleatorias etc. \n",
    "dataloader = DataLoader(dataset=set_datos, batch_size=2, shuffle=False) # Se puede poner num_workers para definir los hilos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El dataloader puede ser iterado como una lista ej. for i in dataloader:\n",
    "iterador = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterador.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asi se puede iterar sobre cada elemento del dataset\n",
    "# Un ciclo asi se puede usar dentro del loop de entrenamiento \n",
    "for i in enumerate(dataloader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset con imagenes\n",
    "Ejemplos de dataset, dataloader y transformaciones sobre set de datos de imagenes\n",
    "Se cuenta con la siguiente estructura de archivos:\n",
    "```python\n",
    "data/\n",
    "├─ etiquetas/\n",
    "│  ├─ train.csv # Archivo con file_name,file_path,class_name,class_index\n",
    "├─ imagenes/\n",
    "│  ├─ seg_train/ # Carpeta con imagenes a procesar\n",
    "│  │  ├─ imagenes_entrenamiento\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 'básico'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Imagenes(torch.utils.data.Dataset): \n",
    "    '''\n",
    "    Clase que inicializa el dataset, tiene una funcion para obtener el tamaño del mismo y una función para extraer ejemplos del set de datos \n",
    "    '''\n",
    "    def __init__(self, archivo_etiquetas, img_dir=\"\", transform=None):\n",
    "        self.etiquetas_df = pd.read_csv(archivo_etiquetas) \n",
    "        self.img_dir = img_dir # Directorio donde estan las imagenes\n",
    "        self.transform = transform # Las transformaciones que se le tienen que hacer a la imagen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.etiquetas_df) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        directorio_imagen = os.path.join(self.img_dir, self.etiquetas_df.iloc[idx, 1]) \n",
    "        imagen = cv2.imread(directorio_imagen) \n",
    "        imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
    "        clase = self.etiquetas_df.iloc[idx, 2] \n",
    "        if self.transform:\n",
    "            imagen = self.transform(imagen)\n",
    "        return imagen, clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dataset\n",
    "set_datos_imagenes = Dataset_Imagenes(archivo_etiquetas='./data/etiquetas/train.csv',\n",
    "                                                img_dir=\"\", transform=None)\n",
    "#Creamos dataloader\n",
    "dataloader_imagenes= DataLoader(set_datos_imagenes, 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "#Accedemos a 5 registros del dataloader\n",
    "for i in enumerate(dataloader_imagenes): \n",
    "    imagen = i[1][0][0]\n",
    "    clase = i[1][1][0]\n",
    "    ax=plt.subplot(2,5,i[0]+1) \n",
    "    ax.title.set_text(clase) \n",
    "    plt.imshow(imagen)\n",
    "    if i[0]==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una funcion con distintas transformaciones\n",
    "transformacion = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.GaussianBlur(13,2),\n",
    "    transforms.Resize((100, 100),\n",
    "    interpolation=PIL.Image.BILINEAR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dataset\n",
    "set_datos_imagenes_t = Dataset_Imagenes(archivo_etiquetas='./data/etiquetas/train.csv', \n",
    "                                img_dir=\"\",\n",
    "                                transform=transformacion)\n",
    "\n",
    "#Creamos dataloader                                \n",
    "dataloader_imagenes_transformadas= DataLoader(set_datos_imagenes_t, 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enumerate(dataloader_imagenes_transformadas): \n",
    "    image = i[1][0][0]\n",
    "    clase = i[1][1][0]\n",
    "    ax=plt.subplot(2,5,i[0]+1) \n",
    "    ax.title.set_text(clase) \n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    if i[0]==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('clasif')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1d81e49e78ec377ecceed83774cfb411f09a4dfaba8425beccc68a5762367e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
